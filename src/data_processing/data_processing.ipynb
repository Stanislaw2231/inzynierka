{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(files):\n",
    "    acc_set = 1\n",
    "    gyr_set = 1\n",
    "\n",
    "    acc_df = pd.DataFrame()\n",
    "    gyr_df = pd.DataFrame()\n",
    "\n",
    "    for f in files:\n",
    "        participant = f.split(\"\\\\\")[1].replace(f, \"\").split(\"-\")[0]\n",
    "        label = f.split(\"-\")[1]\n",
    "        category = f.split(\"-\")[2].rstrip(\"123\")\n",
    "\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "        df[\"participant\"] = participant\n",
    "        df[\"label\"] = label\n",
    "        df[\"category\"] = category\n",
    "\n",
    "        if \"Accelerometer\" in f:\n",
    "            df[\"set\"] = acc_set\n",
    "            acc_set += 1\n",
    "            acc_df = pd.concat([acc_df, df])\n",
    "\n",
    "        if \"Gyroscope\" in f:\n",
    "            df[\"set\"] = gyr_set\n",
    "            gyr_set += 1\n",
    "            gyr_df = pd.concat([gyr_df, df])\n",
    "            \n",
    "    acc_df.index = pd.to_datetime(acc_df[\"epoch (ms)\"], unit=\"ms\")\n",
    "    gyr_df.index = pd.to_datetime(gyr_df[\"epoch (ms)\"], unit=\"ms\")\n",
    "    \n",
    "    \n",
    "    del acc_df[\"epoch (ms)\"]\n",
    "    del acc_df[\"time (01:00)\"]\n",
    "    del acc_df[\"elapsed (s)\"]       \n",
    "    del gyr_df[\"epoch (ms)\"]\n",
    "    del gyr_df[\"time (01:00)\"]\n",
    "    del gyr_df[\"elapsed (s)\"]\n",
    "    \n",
    "    return acc_df, gyr_df\n",
    "\n",
    "files = glob(\"../../data/raw/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(files = files):\n",
    "    acc_df, gyr_df = read_data(files)\n",
    "    df_mergerd = pd.concat([acc_df.iloc[:,:3], gyr_df], axis=1)\n",
    "\n",
    "    df_mergerd.columns = [\n",
    "        \"acc_x\",\n",
    "        \"acc_y\",\n",
    "        \"acc_z\",\n",
    "        \"gyr_x\",\n",
    "        \"gyr_y\",\n",
    "        \"gyr_z\",\n",
    "        \"participant\",\n",
    "        \"label\",\n",
    "        \"category\",\n",
    "        \"set\"\n",
    "    ]\n",
    "\n",
    "    sampling_rules = {\n",
    "        \"acc_x\": \"mean\",\n",
    "        \"acc_y\": \"mean\",\n",
    "        \"acc_z\": \"mean\",\n",
    "        \"gyr_x\": \"mean\",\n",
    "        \"gyr_y\": \"mean\",\n",
    "        \"gyr_z\": \"mean\",\n",
    "        \"participant\": \"last\",\n",
    "        \"label\": \"last\",\n",
    "        \"category\": \"last\",\n",
    "        \"set\": \"last\"\n",
    "    }\n",
    "\n",
    "    days = [g for n, g in df_mergerd.groupby(pd.Grouper(freq=\"D\"))]\n",
    "    df_resampled = pd.concat([df.resample(\"150ms\").apply(sampling_rules).dropna() for df in days])\n",
    "\n",
    "    df_resampled[\"set\"] = df_resampled[\"set\"].astype(\"int\")\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(output_path):\n",
    "    df_resampled = process_data()\n",
    "    df_resampled.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"../../data/partially processed/resampled_data.csv\"\n",
    "export_data(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "\n",
    "# Convert categorical data to numerical data\n",
    "df_encoded = df.copy()\n",
    "df_encoded['participant'] = df_encoded['participant'].astype('category').cat.codes\n",
    "df_encoded['label'] = df_encoded['label'].astype('category').cat.codes\n",
    "df_encoded['category'] = df_encoded['category'].astype('category').cat.codes\n",
    "\n",
    "# Calculate the correlation matrix for the encoded dataframe\n",
    "corr_matrix_encoded = df_encoded.corr()\n",
    "\n",
    "# Plot the heatmap for the encoded correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix_encoded, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix (Encoded)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df_encoded.select_dtypes(include=['float64']).columns\n",
    "for col in numeric_cols:\n",
    "    Q1 = df_encoded[col].quantile(0.05)\n",
    "    Q3 = df_encoded[col].quantile(0.95)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df_encoded = df_encoded[(df_encoded[col] >= lower_bound) & (df_encoded[col] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Prepare the data\n",
    "X = df_encoded.drop(columns=['label'])\n",
    "y = df_encoded['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../data/partially processed/resampled_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../../data/partially processed/resampled_data.csv\")\n",
    "\n",
    "data = df[[\"acc_x\", \"acc_y\", \"acc_z\", \"gyr_x\", \"gyr_y\", \"gyr_z\"]]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
    "labels = dbscan.fit_predict(scaled_data)\n",
    "\n",
    "# Mark clusters and outliers\n",
    "df[\"cluster\"] = labels\n",
    "outliers = df[df[\"cluster\"] == -1]\n",
    "\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# ...existing code...\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv(\"F:/Studia/InÅ¼ynierka/inzynierka/data/partially processed/resampled_data.csv\")\n",
    "label_column = 'label'\n",
    "numeric_data = df.select_dtypes(include=[np.number]).fillna(0)\n",
    "\n",
    "# Scale numeric data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numeric_data)\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.1, min_samples=10)\n",
    "labels = dbscan.fit_predict(scaled_data)\n",
    "\n",
    "# Store cluster labels (-1 indicates outliers)\n",
    "df['outlier'] = labels\n",
    "\n",
    "# Plot by label, coloring points by outlier status\n",
    "for label_val in df[label_column].unique():\n",
    "    subset = df[df[label_column] == label_val]\n",
    "    for col in numeric_data.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(subset.index, subset[col], c=subset['outlier'], cmap='coolwarm')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel(col)\n",
    "        plt.title(f'Label: {label_val} - Outliers detected in {col}')\n",
    "        plt.show()\n",
    "\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# ...existing code...\n",
    "\n",
    "def chauvenet_outliers(values):\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values)\n",
    "    N = len(values)\n",
    "    z = np.abs((values - mean_val) / (std_val if std_val != 0 else 1e-10))\n",
    "    prob_two_tailed = 2 * (1 - norm.cdf(z))\n",
    "    return prob_two_tailed < (1.0 / (2 * N))\n",
    "\n",
    "df = pd.read_csv(\"F:/Studia/InÅ¼ynierka/inzynierka/data/partially processed/resampled_data.csv\")\n",
    "label_column = 'label'\n",
    "numeric_data = df.select_dtypes(include=[np.number]).fillna(0)\n",
    "\n",
    "for label_val in df[label_column].unique():\n",
    "    subset = df[df[label_column] == label_val]\n",
    "    for col in numeric_data.columns:\n",
    "        outlier_mask = chauvenet_outliers(subset[col])\n",
    "        c_vals = outlier_mask.map({True: 1, False: 0})  # color mapping\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(subset.index, subset[col], c=c_vals, cmap='coolwarm')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel(col)\n",
    "        plt.title(f'Label: {label_val} - Outliers detected in {col} (Chauvenet)')\n",
    "        plt.show()\n",
    "\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "def chauvenet_outliers(values, factor=1.0):\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values)\n",
    "    N = len(values)\n",
    "    z = np.abs((values - mean_val) / (std_val if std_val != 0 else 1e-10))\n",
    "    prob_two_tailed = 2 * (1 - norm.cdf(z))\n",
    "    # Adjust detection via factor (default = 1.0)\n",
    "    return prob_two_tailed < (factor / (2 * N))\n",
    "\n",
    "df = pd.read_csv(\"F:/Studia/InÅ¼ynierka/inzynierka/data/partially processed/resampled_data.csv\")\n",
    "label_column = 'label'\n",
    "factor = 1.2  # Adjust this for stricter or looser outlier detection\n",
    "\n",
    "# Identify Chauvenet outliers\n",
    "numeric_data_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[\"outlier\"] = False\n",
    "for col in numeric_data_cols:\n",
    "    outlier_mask = chauvenet_outliers(df[col], factor=factor)\n",
    "    df.loc[outlier_mask, \"outlier\"] = True\n",
    "\n",
    "# Drop outliers\n",
    "df_no_outliers = df[~df[\"outlier\"]]\n",
    "\n",
    "# Optional: plotting\n",
    "for label_val in df_no_outliers[label_column].unique():\n",
    "    subset = df_no_outliers[df_no_outliers[label_column] == label_val]\n",
    "    for col in numeric_data_cols:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(subset.index, subset[col], c='blue', label='Data')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel(col)\n",
    "        plt.title(f'Label: {label_val} - Chauvenet Filtered {col}')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "def chauvenet_outliers(values):\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values)\n",
    "    N = len(values)\n",
    "    z = np.abs((values - mean_val) / (std_val if std_val != 0 else 1e-10))\n",
    "    prob_two_tailed = 2 * (1 - norm.cdf(z))\n",
    "    return prob_two_tailed < (1.0 / (2 * N))\n",
    "\n",
    "def detect_outliers(df):\n",
    "    columns_to_check = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\"]\n",
    "    df[\"outlier\"] = False\n",
    "    for col in columns_to_check:\n",
    "        outlier_mask = chauvenet_outliers(df[col])\n",
    "        df.loc[outlier_mask, \"outlier\"] = True\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(\"F:/Studia/InÅ¼ynierka/inzynierka/data/partially processed/resampled_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_outliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOF_outliers(df, k=20):\n",
    "    columns_to_check = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\"]\n",
    "    features = df[columns_to_check]\n",
    "    lof = LocalOutlierFactor(n_neighbors=k)\n",
    "    outlier_labels = lof.fit_predict(features)\n",
    "    lof_scores = lof.negative_outlier_factor_\n",
    "\n",
    "    df['outlier_label'] = outlier_labels\n",
    "    df['lof_score'] = lof_scores\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers(df, columns_to_check):\n",
    "    # Plot each feature against the outlier label\n",
    "    for col in columns_to_check:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x=df.index, y=df[col], hue=df['outlier_label'], palette='coolwarm')\n",
    "        plt.title(f'Outlier Detection in {col}')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel(col)\n",
    "        plt.legend(title='Outlier Label')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def detect_outliers(file_path, columns_to_check, n_neighbors=10):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Select specified columns and drop NaN values\n",
    "    features = df[columns_to_check].dropna()\n",
    "\n",
    "    # Apply LOF\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors)\n",
    "    outlier_labels = lof.fit_predict(features)\n",
    "    lof_scores = lof.negative_outlier_factor_\n",
    "\n",
    "    # Store results\n",
    "    df['outlier_label'] = outlier_labels\n",
    "    df['lof_score'] = lof_scores\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_outliers(df, columns_to_check):\n",
    "    # Plot each feature against the outlier label\n",
    "    for col in columns_to_check:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x=df.index, y=df[col], hue=df['outlier_label'], palette='coolwarm')\n",
    "        plt.title(f'Outlier Detection in {col}')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel(col)\n",
    "        plt.legend(title='Outlier Label')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "file_path = \"f:/Studia/InÅ¼ynierka/inzynierka/data/partially processed/resampled_data.csv\"\n",
    "columns_to_check = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\"]\n",
    "result_df = detect_outliers(file_path, columns_to_check)\n",
    "result_df\n",
    "# Plot the results\n",
    "plot_outliers(result_df, columns_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"f:/Studia/InÅ¼ynierka/inzynierka/data/partially processed/resampled_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df['category'].unique()\n",
    "df['category'] = df['category'].str.replace(r'(_MetaWear_2019|\\d+)', '', regex=True)\n",
    "df['category'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"f:/Studia/InÅ¼ynierka/inzynierka/data/partially processed/resampled_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import seaborn as sns\n",
    "\n",
    "def LOF_outliers(df, k=20):\n",
    "    columns_to_check = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\"]\n",
    "    features = df[columns_to_check]\n",
    "    lof = LocalOutlierFactor(n_neighbors=k)\n",
    "    outlier_labels = lof.fit_predict(features)\n",
    "    # lof_scores = lof.negative_outlier_factor_\n",
    "\n",
    "    df['outlier_label'] = outlier_labels\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(\"../../data/partially processed/resampled_data.csv\")\n",
    "df_no_outliers = df.copy()\n",
    "columns_to_check = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\"]\n",
    "for col in columns_to_check:\n",
    "    for label in df[\"label\"].unique():\n",
    "        data = LOF_outliers(df[df[\"label\"] == label], col)\n",
    "        \n",
    "        data.loc[data[col + \"_outlier\"] == -1, col] = np.nan\n",
    "        \n",
    "        df_no_outliers.loc[df_no_outliers[\"label\"] == label, col] = data[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def LOF_outliers(df, k=20):\n",
    "    columns_to_check = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\"]\n",
    "    features = df[columns_to_check]\n",
    "    \n",
    "    lof = LocalOutlierFactor(n_neighbors=k)\n",
    "    outlier_labels = lof.fit_predict(features)\n",
    "    df['outlier_label'] = outlier_labels\n",
    "    \n",
    "    for col in columns_to_check:\n",
    "        df.loc[df['outlier_label'] == -1, col] = np.nan\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def LOF_outliers(df, k=20):\n",
    "    columns_to_check = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\"]\n",
    "    features = df[columns_to_check]\n",
    "    \n",
    "    lof = LocalOutlierFactor(n_neighbors=k)\n",
    "    outlier_labels = lof.fit_predict(features)\n",
    "    df['outlier_label'] = outlier_labels\n",
    "    # Plot each feature against the outlier label\n",
    "    for col in columns_to_check:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x=df.index, y=df[col], hue=df['outlier_label'], palette='coolwarm')\n",
    "        plt.title(f'Outlier Detection in {col}')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel(col)\n",
    "        plt.legend(title='Outlier Label')\n",
    "        plt.show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/partially processed/resampled_data.csv\")\n",
    "LOF_outliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../data/partially processed/resampled_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['epoch (ms)', 'acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z',\n",
       "       'participant', 'label', 'category', 'set'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bench' 'ohp' 'squat' 'dead' 'row' 'rest']\n",
      "['heavy' 'medium' 'sitting' 'standing']\n",
      "['B' 'A' 'E' 'C' 'D']\n"
     ]
    }
   ],
   "source": [
    "df.head(5).to_csv(\"sample.csv\")\n",
    "print(df['label'].unique())\n",
    "print(df['category'].unique())\n",
    "print(df['participant'].unique())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
